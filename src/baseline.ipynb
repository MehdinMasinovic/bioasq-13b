{
 "cells": [
  {
   "cell_type": "code",
   "id": "9a1fbb59",
   "metadata": {},
   "source": [
    "# Importing necessary libraries\n",
    "import requests\n",
    "import json\n",
    "import string\n",
    "\n",
    "# Importing NLTK for text processing (if we don't use lemmatization or word2vec)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "929bceaf",
   "metadata": {},
   "source": [
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# Download nltk stopwords and punkt tokenizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1af1cf5e",
   "metadata": {},
   "source": [
    "## Loading training data"
   ]
  },
  {
   "cell_type": "code",
   "id": "5d1db5d0",
   "metadata": {},
   "source": [
    "# Load BioASQ data\n",
    "bioasq_13b_questions = (json.load(open('data/BioASQ-training13b/training13b.json')))['questions']\n",
    "\n",
    "# Extract body, type, and id from the questions\n",
    "bioasq_13b_questions = [\n",
    "    {\n",
    "        # Question data\n",
    "        'body': question['body'],\n",
    "        'type': question['type'],\n",
    "        'id': question['id'],\n",
    "        'target_documents': question['documents'],\n",
    "    }\n",
    "    for question in bioasq_13b_questions\n",
    "    if question['type'] in ['yesno', 'factoid', 'summary', 'list']\n",
    "]\n",
    "\n",
    "# get the first X questions\n",
    "testing_questions = 1\n",
    "bioasq_13b_questions = bioasq_13b_questions[:testing_questions]\n",
    "\n",
    "# Check the number of questions\n",
    "print(f\"Number of BioASQ 13b questions: {len(bioasq_13b_questions)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fd7cdd0f",
   "metadata": {},
   "source": [
    "## PubMed's API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce198331",
   "metadata": {},
   "source": [
    "### Generate session ID"
   ]
  },
  {
   "cell_type": "code",
   "id": "c9ade6b0",
   "metadata": {},
   "source": [
    "GET_SESSION_URL = \"http://bioasq.org:8000/pubmed\"\n",
    "\n",
    "def get_session():\n",
    "    \"\"\"\n",
    "    This function retrieves a session ID from the BioASQ server as a URL.\n",
    "    These session IDs can be used for multiple requests but expire after 10 minutes, \n",
    "    so they must be renewed periodically.\n",
    "\n",
    "    Returns:\n",
    "        str: The session ID as a string (e.g., http://bioasq.org:8000/2?-3a641fde%3A19687315e96%3A-7fe2) if the request is successful, None otherwise.\n",
    "    Raises:\n",
    "        requests.RequestException: If the GET request fails due to network issues or server errors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Sending a GET request to the server\n",
    "        response = requests.get(GET_SESSION_URL)\n",
    "        \n",
    "        # Checking if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Extracting the session ID from the response\n",
    "            return str(response.text)\n",
    "        else:\n",
    "            print(f\"Error: Received status code {response.status_code}\")\n",
    "            return None\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        raise e"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b3dfdfed",
   "metadata": {},
   "source": [
    "### Get list of most relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "id": "82d21a27",
   "metadata": {},
   "source": [
    "def get_most_relevant_documents(keywords, page=0, documents_per_page=25):\n",
    "    \"\"\"\n",
    "    This function retrieves the most relevant documents from the BioASQ server based on the provided keywords.\n",
    "\n",
    "    Args:\n",
    "        keywords (str): The keywords to search for in the documents.\n",
    "        page (int): The page number for pagination. Default is 0.\n",
    "        documents_per_page (int): The number of documents to retrieve per page. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of objects containing the most relevant documents.\n",
    "            Content of the objects:\n",
    "                year (string): The year of publication.\n",
    "                documentAbstract (string): Abstract of the document.\n",
    "                meshAnnotations (unclear - Null): MESH annotations of the document. (No idea what this is, usually Null)\n",
    "                pmid (string): The PubMed ID of the document. Useful in case you want to look for the entire document in PubMed.\n",
    "                        E.g. pmid = 38939119; https://pubmed.ncbi.nlm.nih.gov/38939119/\n",
    "                title (string): Title of the document.\n",
    "                sections (unclear - Null: section of the document? (No idea what this is, usually Null)\n",
    "                fulltextAvailable (Boolean): Indicates if the full text of the document is available.\n",
    "                journal (string): Journal in which the document was published?\n",
    "                meshHeading (list of strings): MESH entities of the document, related to knowledge graphs?\n",
    "\n",
    "            \n",
    "\n",
    "    \"\"\"\n",
    "    session_url = get_session()\n",
    "    request_data = f'json={{\"findPubMedCitations\": [\"{keywords}\", {page}, {documents_per_page}]}}'\n",
    "    \n",
    "    response = requests.post(session_url, data=request_data)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()['result']['documents']\n",
    "    else:\n",
    "        print(f\"Error: Received status code {response.status_code}\")\n",
    "        return None\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ea062f0f",
   "metadata": {},
   "source": [
    "## Traditional IR model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaaa825",
   "metadata": {},
   "source": [
    "### Step 1: Extract keywords from questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea21062",
   "metadata": {},
   "source": [
    "#### Remove stop words and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "id": "4c9529fc",
   "metadata": {},
   "source": [
    "def extract_keywords(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Filter stopwords and punctuation\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    keywords = [\n",
    "        word for word in tokens \n",
    "        if word.isalnum() and word not in stop_words\n",
    "    ]\n",
    "    # Return keywords as a string\n",
    "    # return ' '.join(keywords)\n",
    "\n",
    "    # Return keywords as a list of strings\n",
    "    return keywords\n",
    "\n",
    "# For each question, extract keywords and save them in a attribute keywords\n",
    "for question in bioasq_13b_questions:\n",
    "    question['keywords'] = extract_keywords(question['body'])\n",
    "\n",
    "print(\"Original question body:\")\n",
    "print(bioasq_13b_questions[0]['body'])\n",
    "print(\"\\nExtracted keywords:\")\n",
    "print(bioasq_13b_questions[0]['keywords'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "343bd797",
   "metadata": {},
   "source": [
    "### Step 2: Consume PubMed's API to get relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "id": "5c57ebe7",
   "metadata": {},
   "source": [
    "# Get the most relevant documents for each question according to the PubMed API\n",
    "# and save them in a new attribute documents_api\n",
    "for question in bioasq_13b_questions:\n",
    "    documents = get_most_relevant_documents(' '.join(question['keywords']))\n",
    "    question['documents_api'] = documents\n",
    "\n",
    "    print(f\"Documents found for question `{question['id']}`: {len(documents)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "74e64fd7",
   "metadata": {},
   "source": [
    "### Step 3: Rank documents with \"Traditional IR\" model"
   ]
  },
  {
   "cell_type": "code",
   "id": "acb6805e",
   "metadata": {},
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "for question in bioasq_13b_questions:\n",
    "\n",
    "    # ---------------- Process the documents ----------------\n",
    "    # For each question, concatenate the title and abstract of each of its document\n",
    "    full_doc = [doc[\"title\"] + \" \" + doc[\"documentAbstract\"] for doc in question['documents_api']]\n",
    "\n",
    "    # Tokenize the full documents (title + abstract) of the question\n",
    "    tokenized_docs = [word_tokenize(doc.lower()) for doc in full_doc]\n",
    "    \n",
    "    # Tokenize the question (question body)\n",
    "    tokenized_question = word_tokenize(question['body'].lower())\n",
    "\n",
    "    # ---------------- Score the documents ----------------\n",
    "    # Create bm25 instance\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "    # Get the scores for the query\n",
    "    scores = bm25.get_scores(tokenized_question)\n",
    "\n",
    "    # Sort documents by score\n",
    "    ranked_docs = sorted(zip(question['documents_api'], scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Keep the top 10 documents\n",
    "    top_docs = [doc for doc, score in ranked_docs[:10]]\n",
    "\n",
    "    print(f\"Ranked documents for question `{question['id']}`:\")\n",
    "    \n",
    "    # Print the top 10 documents id with their scores\n",
    "    for doc, score in ranked_docs[:10]:\n",
    "        print(f\"Document ID: {doc['pmid']}, Score: {score}\")\n",
    "    print(\"\\n\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e238e0d3339fbb06",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
